<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>http://localhost:4000</link>
    <description>
      A blog about Data Science, Machine Learning, and Artificial Intelligence.
    </description>
    
        
            <item>
                <title>To embed or not to embed: the question for sentiment analysis</title>
                <link>http://localhost:4000/2018/08/03/to-embed-or-not-the-question-for-sentiment-analysis/</link>
                <content:encoded>
                    <![CDATA[
                    <p>Word embeddings are very popular for NLP tasks. Some data scientists use pretrained “out-of-the-box” word embeddings to classify sentiment. However, it was <a href="https://github.com/attardi/deepnl/wiki/Sentiment-Specific-Word-Embeddings">pointed out</a> that generic word embeddings produce vector space representations that do not necessary well encode sentiment information by proximity relationships. To cite the <em>deepnl</em> Python package documentation:</p>

<blockquote>
  <p>Word embeddings are typically learned from unannotated plain text and provide a dense vector representation of syntactic/semantic aspects of a word. These representations though are not able to distinguish contrasting aspects of a word sense, for example sentiment polarity or opposite senses (e.g. high/low).</p>
</blockquote>

<p>In this post, we are going to verify this claim and compare several pretrained embeddings in a sentiment analysis task.</p>

<h1 id="our-setup">Our setup</h1>

<ul>
  <li>
    <p>Data: Opinion lexicon dataset  by Liu and Hu <a href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon">https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon</a>.</p>

    <p>The dataset contains ~7000 English words with positive and negative sentiment (binary labels)</p>
  </li>
  <li>
    <p>Several pre-trained word embeddings: Google’s Word2Vec, Glove, MUSE.</p>
  </li>
</ul>

<h2 id="choice-of-appropriate-text-data-representation-method">Choice of appropriate text data representation method</h2>

<p>What is the most efficient method to numerically represent text data to carry out text classification?</p>

<h3 id="classical-approach-docs-as-vectors">Classical approach: docs as vectors</h3>

<p>In text classification, a standard data preprocessing step is to represent text documents as numerical vectors. For example, the almighty bag-of-words (BOW) model represents a text document with $(c_1 \ c_2\ …\ c_n)^\text{T} \in \mathbb{R}^n$ where $c_i$ is the number of occurences of the word $i$, $n$ is the size of the vocabulary. A slightly more complicated version of BOW is to replace raw word count with TF-IDF (Term Frequency times Inverse Document Frequency) whereby the frequency of a word occurence in a document is normalized by its frequency across documents.</p>

<h3 id="zoo-of-embeddings-words-as-vectors">Zoo of embeddings: words as vectors</h3>

<p>Instead of representing whole documents as vectors one can represent single words. A simplest way to do it is one-hot encoding vector for vocabulary of size $n$: a word $j \in \{1…n\}$ is represented by $(b_1\ b_2\ …\ b_n)^\text{T}$ where all $b_i = \delta_{ij}$.
After one has a vector space representation, one can learn mapping from the vector space with one dimension per word to a lower dimensional continuous vector space. To learn this embedding one can construct objective functions so that the learned mapping reflects syntactic relationshits between words. For this purpuse, neural network based embeddings become increasingly popular: <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> by Google, <a href="https://research.fb.com/downloads/muse-multilingual-unsupervised-and-supervised-embeddings/">MUSE</a> by facebook, <a href="https://en.wikipedia.org/wiki/GloVe">GloVe</a> at Stanford, FastText. The advantage of these more modern approaches is that they can learn syntactic relationships that allow for meaningful word-vector distances.</p>

<h1 id="whats-up-with-embeddings">What’s up with embeddings?</h1>

<p>In this post we wish to see how word sentiment is reflected in the structure of embedding word-vector spaces of popular embedding models. For this purpose we will use sentiment-specific words of the English language and compare how well words with opposite polarity are separated in embedding space for pretrained GloVe, Word2Vec and MUSE.</p>

<p>But first, some imports:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">scipy.spatial.distance</span> <span class="k">as</span> <span class="n">distance</span>
<span class="kn">from</span> <span class="nn">statistics</span> <span class="kn">import</span> <span class="n">mode</span>

<span class="c"># to compute KS distance between distributions:</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">ks_2samp</span> 

<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>
<span class="kn">from</span> <span class="nn">pandas.plotting</span> <span class="kn">import</span> <span class="n">radviz</span>

<span class="kn">import</span> <span class="nn">gensim</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="k">try</span><span class="p">:</span>
    <span class="c"># https://github.com/DmitryUlyanov/Multicore-TSNE:</span>
    <span class="kn">from</span> <span class="nn">MulticoreTSNE</span> <span class="kn">import</span> <span class="n">MulticoreTSNE</span> <span class="k">as</span> <span class="n">TSNE</span>
<span class="k">except</span> <span class="nb">ImportError</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Using sklearn</span><span class="se">\'</span><span class="s">s t-SNE implementation'</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
    
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># nice plots in jupyter notebooks</span>
<span class="kn">import</span> <span class="nn">pylab</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">pylab</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">8</span>
</code></pre></div></div>

<h2 id="downloading-embeddings-can-place-under-a-cut-in-a-post">Downloading embeddings (can place under a cut in a post)</h2>

<p>Pre-trained MUSE and GloVe in plain-text format are accessible for direct download from the web via <em>wget</em> or <em>curl</em>.</p>

<p>A binary file of the word2vec model trained on the Google news corpus is hosted on Google Drive: <a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit">https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit</a></p>

<p>Below is a simple Python routine to download the file from Google Drive (working as of August 2018):</p>

<details>
  <summary>
    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Google downloader
</code></pre></div>    </div>
  </summary>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">requests</span>

<span class="k">def</span> <span class="nf">download_file_from_google_drive</span><span class="p">(</span><span class="nb">id</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">get_confirm_token</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">cookies</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s">'download_warning'</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">value</span>

        <span class="k">return</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">save_response_content</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
        <span class="n">CHUNK_SIZE</span> <span class="o">=</span> <span class="mi">32768</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">iter_content</span><span class="p">(</span><span class="n">CHUNK_SIZE</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">chunk</span><span class="p">:</span> <span class="c"># filter out keep-alive new chunks</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>

    <span class="n">URL</span> <span class="o">=</span> <span class="s">'https://docs.google.com/uc?export=download'</span>

    <span class="n">session</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">URL</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="p">{</span> <span class="s">'id'</span> <span class="p">:</span> <span class="nb">id</span> <span class="p">},</span> <span class="n">stream</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">get_confirm_token</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">token</span><span class="p">:</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span> <span class="s">'id'</span> <span class="p">:</span> <span class="nb">id</span><span class="p">,</span> <span class="s">'confirm'</span> <span class="p">:</span> <span class="n">token</span> <span class="p">}</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">URL</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">,</span> <span class="n">stream</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

    <span class="n">save_response_content</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">destination</span><span class="p">)</span>    
</code></pre></div>  </div>

</details>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">FILE_ID</span> <span class="o">=</span> <span class="s">'0B7XkCwpI5KDYNlNUTTlSS21pQmM'</span>

<span class="n">download_file_from_google_drive</span><span class="p">(</span><span class="n">FILE_ID</span><span class="p">,</span>
                                <span class="s">'./data/GoogleNews-vectors-negative300.bin.gz'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="loading-word-embeddings">Loading word embeddings</h2>

<p>We load MUSE embeddings that output a 300-dimensional real-valued vector, taking a token as an input. The pre-trained MUSE model can be downloaded from <a href="https://github.com/facebookresearch/MUSE">https://github.com/facebookresearch/MUSE</a>, and is essentially a plain-text representation of the word-vector dictionary.
We also loaded GloVe and word2vec trained on Google news (download from <a href="http://nlp.stanford.edu/data/glove.twitter.27B.zip">http://nlp.stanford.edu/data/glove.twitter.27B.zip</a>, <a href="https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_embedding</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> 
                   <span class="n">full_vocab</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="s">'''Load a word-vector dictionary from plain text'''</span>
    
    <span class="n">embedding_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span>
              <span class="n">encoding</span><span class="o">=</span><span class="s">'utf-8'</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">,</span>
              <span class="n">errors</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span> <span class="k">as</span> <span class="n">datafile</span><span class="p">:</span>
        
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">datafile</span><span class="p">:</span>
            <span class="n">word</span><span class="p">,</span> <span class="n">vector</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">full_vocab</span><span class="p">:</span>
                <span class="n">word</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
            <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromstring</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">' '</span><span class="p">)</span>
            <span class="n">embedding_dict</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
    
    <span class="n">mode_len</span> <span class="o">=</span> <span class="n">mode</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span> <span class="k">for</span> <span class="n">vector</span> <span class="ow">in</span> <span class="n">embedding_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()])</span>
    <span class="n">embedding_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">vector</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">vector</span> 
                      <span class="ow">in</span> <span class="n">embedding_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span> <span class="o">==</span> <span class="n">mode_len</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">embedding_dict</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c"># loading MUSE</span>
<span class="n">embeddings</span><span class="p">[</span><span class="s">'muse'</span><span class="p">]</span> <span class="o">=</span> <span class="n">load_embedding</span><span class="p">(</span><span class="s">'./data/muse_trained/wiki.multi.en.vec'</span><span class="p">)</span>

<span class="c"># loading Word2Vec</span>
<span class="n">w2v_dim</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">embeddings</span><span class="p">[</span><span class="s">'word2vec'</span><span class="p">]</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span>
                            <span class="s">'./data/w2v_trained/GoogleNews-vectors-negative300.bin'</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">embeddings</span><span class="p">[</span><span class="s">'word2vec'</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="s">'word2vec'</span><span class="p">]</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">embeddings</span><span class="p">[</span><span class="s">'word2vec'</span><span class="p">]</span><span class="o">.</span><span class="n">vectors</span><span class="p">))</span>
<span class="n">embeddings</span><span class="p">[</span><span class="s">'word2vec'</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">vector</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="p">[</span><span class="s">'word2vec'</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> 
                          <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span> <span class="o">==</span> <span class="n">w2v_dim</span> <span class="ow">and</span> <span class="n">word</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">islower</span><span class="p">()}</span>

<span class="c"># loading GloVe</span>
<span class="n">embeddings</span><span class="p">[</span><span class="s">'glove'</span><span class="p">]</span> <span class="o">=</span> <span class="n">load_embedding</span><span class="p">(</span><span class="s">'./data/glove_trained/glove.twitter.27B.200d.txt'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CPU times: user 2min 12s, sys: 4.92 s, total: 2min 17s
Wall time: 2min 18s
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># let's see the dimensionality of our embedding word-vectors:</span>

<span class="n">test_word</span> <span class="o">=</span> <span class="s">'perestroika'</span>

<span class="n">embeddings</span><span class="p">[</span><span class="s">'muse'</span><span class="p">][</span><span class="n">test_word</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> \
<span class="n">embeddings</span><span class="p">[</span><span class="s">'word2vec'</span><span class="p">][</span><span class="n">test_word</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> \
<span class="n">embeddings</span><span class="p">[</span><span class="s">'glove'</span><span class="p">][</span><span class="n">test_word</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((300,), (300,), (200,))
</code></pre></div></div>

<h2 id="separation-of-labeled-sentiment-specific-words-in-the-embedding-space">Separation of labeled sentiment-specific words in the embedding space</h2>

<p>We will use a dataset of sentiment-specific words together with embeddings to see how well these words are separated inthe embedding space.</p>

<p>Let’s use the <a href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon">Liu and Hu opinion lexicon</a> dataset for our tests. The dataset contains around 6800 positive and negative opinion words or sentiment words for the English language. The dataset is not large, but it will give us an intuition of how sentiment-specific words are structured in embedding spaces.</p>

<p><a href="https://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar">direct download link</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_words_liu</span><span class="p">(</span><span class="n">folder</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">file_to_list</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
        <span class="n">header_lines</span> <span class="o">=</span> <span class="mi">35</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'latin-1'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span> 
                    <span class="k">if</span> <span class="n">index</span> <span class="o">&gt;=</span> <span class="n">header_lines</span><span class="p">]</span>
    
    <span class="n">list_positive</span> <span class="o">=</span> <span class="n">file_to_list</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> 
                                              <span class="s">'positive-words.txt'</span><span class="p">))</span>
    <span class="n">list_negative</span> <span class="o">=</span> <span class="n">file_to_list</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> 
                                              <span class="s">'negative-words.txt'</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="p">{</span><span class="s">'pos'</span><span class="p">:</span> <span class="nb">set</span><span class="p">(</span><span class="n">list_positive</span><span class="p">),</span> 
            <span class="s">'neg'</span><span class="p">:</span> <span class="nb">set</span><span class="p">(</span><span class="n">list_negative</span><span class="p">)}</span>

<span class="n">words</span> <span class="o">=</span> <span class="n">load_words_liu</span><span class="p">(</span><span class="s">'./data/opinion_lexicon/'</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s limit the amount of both positive and negative sentiment words in our data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">words_num</span> <span class="o">=</span> <span class="mi">2000</span>     
<span class="c"># a part of these words would not be encoded by embedding models</span>
<span class="c"># simply due to absense of the word in the initial corpus</span>
    
<span class="n">words</span> <span class="o">=</span> <span class="p">{</span><span class="n">sentiment</span><span class="p">:</span> <span class="nb">set</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">word_list</span><span class="p">)[:</span><span class="n">words_num</span><span class="p">])</span> 
         <span class="k">for</span> <span class="n">sentiment</span><span class="p">,</span> <span class="n">word_list</span> <span class="ow">in</span> <span class="n">words</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></div>

<p>Let’s define a function that embeds all words from a list of words</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">embed_from_list</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">word_list</span><span class="p">):</span>
    
    <span class="n">list_embedded</span> <span class="o">=</span> <span class="p">[</span><span class="n">embedding</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span>
                     <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">embedding</span><span class="p">]</span>
    
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">list_embedded</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">list_embedded</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span>
</code></pre></div></div>

<p>We now embed all words corresponding to positive and negaive classes with all embedding models.</p>

<p>If we plot the pairwise distance distributions for the two classes of words, we will see that althouth distributions largely overlap, separation between the negative and positive word-vector clouds exists and differs across embeddings: better separation can be seen for Word2Vec and MUSE than for GloVe (a fact also confirmed by <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov-Smirnov</a> statistics).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    
    <span class="c"># Pairwise distance histograms:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">pairwise_dist_distrib</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="k">for</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'pos'</span><span class="p">,</span> <span class="s">'neg'</span><span class="p">]:</span>
        
        <span class="n">pairwise_dist_distrib</span><span class="p">[</span><span class="n">sentiment</span><span class="p">]</span> <span class="o">=</span> \
            <span class="n">distance</span><span class="o">.</span><span class="n">pdist</span><span class="p">(</span><span class="n">embed_from_list</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">words</span><span class="p">[</span><span class="n">sentiment</span><span class="p">]))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pairwise_dist_distrib</span><span class="p">[</span><span class="n">sentiment</span><span class="p">],</span>
                 <span class="n">label</span><span class="o">=</span><span class="n">sentiment</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Inter-vector distance'</span><span class="p">)</span>
    
    <span class="n">pairwise_dist_distrib</span><span class="p">[</span><span class="s">'pos-neg'</span><span class="p">]</span> <span class="o">=</span> \
        <span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">embed_from_list</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">words</span><span class="p">[</span><span class="s">'pos'</span><span class="p">]),</span>
                       <span class="n">embed_from_list</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">words</span><span class="p">[</span><span class="s">'neg'</span><span class="p">]))</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pairwise_dist_distrib</span><span class="p">[</span><span class="s">'pos-neg'</span><span class="p">],</span>
             <span class="n">label</span><span class="o">=</span><span class="s">'pos-neg'</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">name</span> <span class="o">+</span> <span class="s">' embedding'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Inter-vector distance'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    
    <span class="c"># KS statistics:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_records</span><span class="p">(</span>
        <span class="p">[</span> <span class="p">[</span><span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">ks_2samp</span><span class="p">(</span><span class="n">pairwise_dist_distrib</span><span class="p">[</span><span class="n">k1</span><span class="p">],</span>
                            <span class="n">pairwise_dist_distrib</span><span class="p">[</span><span class="n">k2</span><span class="p">])[</span><span class="mi">0</span><span class="p">]]</span>
          <span class="k">for</span> <span class="n">k1</span> <span class="ow">in</span> <span class="n">pairwise_dist_distrib</span>
             <span class="k">for</span> <span class="n">k2</span> <span class="ow">in</span> <span class="n">pairwise_dist_distrib</span>
                 <span class="k">if</span> <span class="n">k1</span> <span class="o">&lt;</span> <span class="n">k2</span>
        <span class="p">],</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'first'</span><span class="p">,</span> <span class="s">'second'</span><span class="p">,</span> <span class="s">'KS-distance'</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>muse
  first   second  KS-distance
0   pos  pos-neg     0.108299
1   neg      pos     0.060788
2   neg  pos-neg     0.167416


word2vec
  first   second  KS-distance
0   pos  pos-neg     0.170442
1   neg      pos     0.179398
2   neg  pos-neg     0.028857


glove
  first   second  KS-distance
0   pos  pos-neg     0.045027
1   neg      pos     0.019457
2   neg  pos-neg     0.059782
</code></pre></div></div>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_21_1.png" alt="png" /></p>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_21_2.png" alt="png" /></p>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_21_3.png" alt="png" /></p>

<p>Let’s define a utility function that converts words of different sentiment to a design matrix and a label vector in a standard <em>sklearn</em> format:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">vectorize</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
    
    <span class="n">data_matrix</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">sentiment</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'pos'</span><span class="p">,</span> <span class="s">'neg'</span><span class="p">]:</span>
        <span class="n">X_</span> <span class="o">=</span> <span class="n">embed_from_list</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> 
                             <span class="n">words</span><span class="p">[</span><span class="n">sentiment</span><span class="p">])</span>
        <span class="n">data_matrix</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X_</span><span class="p">)</span>
        <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">sentiment</span><span class="p">]</span> <span class="o">*</span> <span class="n">X_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">data_matrix</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s define a function that plots 2-dimensional scatter plots of word-vectors after a dimensionality reduction technique is applied to the data (e.g. 2-component PCA transform):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_lowdim_embed</span><span class="p">(</span><span class="n">dim_reducer_</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">,</span>
                      <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                      <span class="n">subsample</span><span class="o">=</span><span class="mi">1000</span> <span class="c"># subsample length for better viz</span>
                      <span class="p">):</span>
    
    <span class="n">fitted_dimreducers</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span>
                            <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span> <span class="mi">5</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">subsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">{</span><span class="n">sentiment</span><span class="p">:</span> <span class="nb">set</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">word_list</span><span class="p">)[:</span><span class="n">subsample</span><span class="p">])</span> 
                 <span class="k">for</span> <span class="n">sentiment</span><span class="p">,</span> <span class="n">word_list</span> <span class="ow">in</span> <span class="n">words</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">vectorize</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
        <span class="n">y_</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="nb">map</span><span class="p">({</span><span class="s">'neg'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> 
                               <span class="s">'pos'</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">dim_reducer</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">dim_reducer_</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">dim_reducer</span> <span class="o">=</span> <span class="n">dim_reducer_</span>

        <span class="n">lowdim_embed</span> <span class="o">=</span> <span class="n">dim_reducer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_</span><span class="p">)</span>
        <span class="n">fitted_dimreducers</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">dim_reducer</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">lowdim_embed</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
                   <span class="n">lowdim_embed</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> 
                    <span class="n">color</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="nb">map</span><span class="p">(</span>
                        <span class="p">{</span><span class="s">'pos'</span><span class="p">:</span> <span class="s">'red'</span><span class="p">,</span>
                         <span class="s">'neg'</span><span class="p">:</span> <span class="s">'blue'</span><span class="p">}),</span>
                    <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">fitted_dimreducers</span>
</code></pre></div></div>

<p>We can apply PCA straightaway (embedded vectors are normalized) and by taking the first 2 principal components we get these plots:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Red points represent positive sentiment, '</span> \
      <span class="s">'blue points - negative sentiment'</span><span class="p">)</span>
<span class="n">plot_lowdim_embed</span><span class="p">(</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">);</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Red points represent positive sentiment, blue points - negative sentiment
</code></pre></div></div>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_27_1.png" alt="png" /></p>

<p>We can also apply a nonlinear mapping such as 2D t-SNE to get a nicer visual separation of sentiment classes:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span> 
<span class="k">print</span><span class="p">(</span><span class="s">'Red points represent positive sentiment, '</span> \
      <span class="s">'blue points - negative sentiment'</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plot_lowdim_embed</span><span class="p">(</span><span class="n">TSNE</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> 
                      <span class="n">embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Red points represent positive sentiment, blue points - negative sentiment
CPU times: user 1min 1s, sys: 364 ms, total: 1min 1s
Wall time: 1min 1s
</code></pre></div></div>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_29_1.png" alt="png" /></p>

<p>Let’s visualize the first 6 principal components of the data with <a href="https://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-radviz">RadViz</a>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span>
                        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">),</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">word_num_cutoff</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">words_</span> <span class="o">=</span> <span class="p">{</span><span class="n">sentiment</span><span class="p">:</span> <span class="nb">set</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">word_list</span><span class="p">)[:</span><span class="n">word_num_cutoff</span><span class="p">])</span> 
          <span class="k">for</span> <span class="n">sentiment</span><span class="p">,</span> <span class="n">word_list</span> <span class="ow">in</span> <span class="n">words</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">embedding</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    
    <span class="n">list_df</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">vectorize</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">words_</span><span class="p">)</span>
    <span class="n">lowdim_embed</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lowdim_embed</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s">'class'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
    <span class="n">radviz</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s">'class'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s">'r'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.08</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_31_0.png" alt="png" /></p>

<p>From the above plots we see that there’s clearly some separation between word-vector high density parts of clouds corresponding to different sentiment polarity. However for a given region in word-vector space where a certain sentiment is dominant, there still exist points belonging to an opposite sentiment class.</p>

<p>Some of the embedding space dimensions are probably more useful in encoding sentiment than others.
One way to detect such dimensions is to evaluate feature importance scores of embedding components in a sentiment classification task.
Here we use logistic regression with an L1 regularization term to classify sentiment of individual words for this purpose (tree-based models are also very useful for this kind of task):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">logit</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span>
                           <span class="n">penalty</span><span class="o">=</span><span class="s">'l1'</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">coefficients</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">vectorize</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
    
    <span class="n">data_split</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                  <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">data_split</span>

    <span class="n">logit</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> 
                               <span class="n">logit</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)),</span> <span class="mi">4</span><span class="p">),</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    
    <span class="n">coefficients</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>muse
0.9256 

word2vec
0.9353 

glove
0.8613 
</code></pre></div></div>

<p>We’ve got good accuracy here, which means that it is possible to separate sentiment specific words with a hyperplane in the embedding space for all three embedding models.</p>

<p>We now wish to check which embedding dimensions were useful in sentiment classification by examining absolute values of logit coefficients:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'Logit with L1 reg coefficient values for different embeddings:</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>

<span class="n">large_coefs</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">coefficients</span><span class="p">),</span> 
                        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">coefficients</span><span class="p">),</span> <span class="mi">5</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">coefs</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">coefficients</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">coefs</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">large_coefs</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span> <span class="o">&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="s">'</span><span class="se">\n</span><span class="s"># of large coefficients '</span><span class="p">,</span> 
          <span class="nb">len</span><span class="p">(</span><span class="n">coefs</span><span class="p">[</span><span class="n">large_coefs</span><span class="p">[</span><span class="n">name</span><span class="p">]]),</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Logit with L1 reg coefficient values for different embeddings:

muse 
# of large coefficients  50 

word2vec 
# of large coefficients  74 

glove 
# of large coefficients  89 
</code></pre></div></div>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_37_1.png" alt="png" /></p>

<p>We can now cosider reduced embedding spaces which are only comprised of embedding dimensions relevant for sentiment classification:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">short_embeddings</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">embedding</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="p">:</span>
    <span class="n">short_embeddings</span><span class="p">[</span><span class="n">embedding</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">vector</span><span class="p">[</span><span class="n">large_coefs</span><span class="p">[</span><span class="n">embedding</span><span class="p">]]</span>
                                   <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="n">embeddings</span><span class="p">[</span><span class="n">embedding</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_word</span> <span class="o">=</span> <span class="s">'perestroika'</span>

<span class="n">short_embeddings</span><span class="p">[</span><span class="s">'muse'</span><span class="p">][</span><span class="n">test_word</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> \
<span class="n">short_embeddings</span><span class="p">[</span><span class="s">'word2vec'</span><span class="p">][</span><span class="n">test_word</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> \
<span class="n">short_embeddings</span><span class="p">[</span><span class="s">'glove'</span><span class="p">][</span><span class="n">test_word</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> \
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>((50,), (74,), (89,))
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot_lowdim_embed</span><span class="p">(</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> 
                  <span class="n">short_embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_41_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span> <span class="o">=</span> <span class="n">plot_lowdim_embed</span><span class="p">(</span><span class="n">TSNE</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> 
                      <span class="n">short_embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_42_0.png" alt="png" /></p>

<p>Compared to the PCA plots for full embedding spaces, there is visually a significant increase in separability, but there is still a considerable overlap between sentiment classes.</p>

<p>We will now try to learn a nonlinear mapping of word vectors to lower-dimensional space such that it maximizes separability of sentiment classes.
We will use a neural network model to learn this mapping.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model_generator</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span>
                    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta_1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)):</span>
    
    <span class="n">inp</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s">'x_input'</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">inp</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'tanh'</span><span class="p">)(</span><span class="n">result</span><span class="p">)</span>    
    <span class="n">embedder</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'mapper'</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">result</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">)(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'classifier'</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inp</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">result</span><span class="p">)</span>
    <span class="n">classifier</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> 
                       <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> 
                       <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">embedder</span><span class="p">,</span> <span class="n">classifier</span>
</code></pre></div></div>

<p>Next we define new dimensionality reduction class based on the neural network model constructed above:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NeuralWordMapping</span><span class="p">():</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                 <span class="n">model_maker</span><span class="o">=</span><span class="n">model_generator</span><span class="p">,</span>
                 <span class="n">verbose_visual</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_maker</span> <span class="o">=</span> <span class="n">model_maker</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose_visual</span> <span class="o">=</span> <span class="n">verbose_visual</span>
        
    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        
        <span class="n">embedder</span><span class="p">,</span> <span class="n">classifier</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_maker</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
                                                <span class="n">embed_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">)</span>
        
        <span class="n">h</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                           <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose_visual</span><span class="p">:</span>
        
            <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Train and validation accuracy vs. training epoch'</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'acc'</span><span class="p">,</span> <span class="s">'val_acc'</span><span class="p">]:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">classifier</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedder</span> <span class="o">=</span> <span class="n">embedder</span>
        
        <span class="k">return</span> <span class="n">embedder</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plot_lowdim_embed</span><span class="p">(</span><span class="n">NeuralWordMapping</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                        <span class="n">verbose_visual</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                      <span class="n">embeddings</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CPU times: user 17.8 s, sys: 432 ms, total: 18.2 s
Wall time: 18.1 s
</code></pre></div></div>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_48_1.png" alt="png" /></p>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_48_2.png" alt="png" /></p>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_48_3.png" alt="png" /></p>

<p><img src="/images/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_files/2018-08-03-to-embed-or-not-the-question-for-sentiment-analysis_48_4.png" alt="png" /></p>

<p>Now the sentiment classes look quite nicely separated compared to the PCA plots, and we were able to achieve more than 80% accuracy in sentiment classification on this dataset for MUSE and word2vec embeddings. Results were slightly worse for GloVe embeddings.</p>

<p>These tests demonstrate that a claim that word embedding spaces carry no information (structure) about word sentiment/polarity is not exactly right, and this structure can be nicely revealed with appropriate dimensionality reduction techniques.</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/2018/08/03/to-embed-or-not-the-question-for-sentiment-analysis/</guid>
                <description>
                    
                </description>
                <pubDate>Fri, 03 Aug 2018 00:00:00 +0200</pubDate>
                <author>Ivan Lazarevich and Ilya Prokin</author>
            </item>
        
    
  </channel>
</rss>
